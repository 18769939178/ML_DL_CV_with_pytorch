# 线性回归
## 背景
让我们通过一个例子来开始：这个例子是预测住房价格的，我们要使用一个数据集，数据集包含很多个俄勒冈州波特兰市住房的价格及各种情况（如房房屋大小，房间数量等等）。然后我们根据一个新房屋的数据去预测它的价格。
## 符号定义
我们定义数据集为
$$D=\{(x^{(i)},y^{(i)}); i = 1,\cdots ,m\}$$
其中$x^{(i)}$ 表示输入的变量值，包括要输入的各种信息，也可以叫做**输入特征、自变量**，是一个向量，有$p$个维度（代表
个特征），每个维度代表一种属性（如房屋大小），$y^{(i)}$ 来表示“输出值”，或者称之为**目标变量、因变量**，这个例子里面就是房屋价格，$m$表示数据集所包含的样本数量，一个$(x^{(i)},y^{(i){}})$被称为一个**训练样本**或者**实例**
其中输入特征为
$$x=\left[ \begin{matrix} x_1 \\ x_2 \\ \vdots \\x_p\end{matrix} \right] $$
然后我们定义模型为线性模型来进行预测
$$y=f(x)$$
$$y=w^Tx+b$$
$h$  代表学习算法的解决方案或函数也称为假设（**hypothesis**）
![](../images/ad0718d6e5218be6e6fce9dc775a38e6.png)
# 梯度下降法
## 单个参数问题
以只带单个参数w的Loss Function $L(w)$为例，首先保证$L(w)$是**可微**的
$$w^*={arg}\ \underset{w}{min} L(w) $$ 我们的目标就是找到这个使Loss最小的$w^*$，实际上就是寻找切线L斜率为0的global minima最小值点(注意，存在一些local minima极小值点，其斜率也是0)

有一个暴力的方法是，穷举所有的w值，去找到使loss最小的$w^*$，但是这样做是没有效率的；而gradient descent就是用来解决这个效率问题的
# 分类
回归的输出是一个单连续数值，是自然区间$\mathbb{R}$的子集，以预测值与真实值之间的区别作为损失，但是碰到某些输出为离散序列的情况就无法进行预测，如某个电子邮件是否属于垃圾邮件文件夹，一个