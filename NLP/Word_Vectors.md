# 介绍

一个单词是可以使用一个很大的实数向量来表示的，同时对于人类来说，语言是容易理解的，但是对于计算机来说，这是很难理解和表达的，或者这与语言是由人类构建的系统这一 事实有关

下图是大语言模型GPT3的效果展示，它可以理解输入文本所包含的意思，并且进行回答，完成一些神奇的任务

![CS224N_L1_12](.\assets\CS224N_L1_12.png)

## 语义

那么问题就来了，模型是如何理解语言的含义的呢，这就涉及我们如何表示一个词的含义了，根据韦氏词典，一个词的**含义（meaning）**是一个词所代表的想法，或者说一个人想通过使用单词符号表达的想法，或者说韦氏词典对单词含义的定义集中在想法这个词上，实际上这很接近语言学家思考意义的最常见方式，他们认为词义是作为能连接语言符号（或者说一个词所表示的符号）与其所表示的具体观念或事物之间的配对，比如说“椅子”这个词表示的就是椅子的集合，这也被称为语义，这种术语也被应用在编程语义的语义中，当然这个模型的实施性不是很深入，比如说很难从“椅子”意味着世界上所有的椅子的集合转换为可以使用计算机操纵的意义

![CS224N_L1_16](.\assets\CS224N_L1_16.png)

所以，一般来说，NLP系统处理语义的方式就是利用字典等资源，特别是同义词库WordNet，这一种广泛使用的英语词汇数据库和语义网络，主要目标是为单词提供定义、同义词和词汇关系，以帮助计算机更好地理解和处理自然语言。它以一种层次结构的方式组织单词，形成词汇网络，其中单词按照它们的语义和词义关系进行分类和连接。在这里，它可以将单词和术语组织成可以表示同一事物的同义词组

![CS224N_L1_17](.\assets\CS224N_L1_17.png)

当然WordNet也有很多问题，比如说：

1. 缺少细微差别，可能无法准确地表示一些具有复杂关系的词语，比如说很难表示相似的东西，导致某些相似的东西不是同义词
2. 缺少单词的新语义，或者说

![CS224N_L1_18](.\assets\CS224N_L1_18.png)

## 单词表示方式

在传统NLP中，会使用**本地化表示（localist representation）**，将单词、短语或句子表示为固定、离散的符号或代码的方式，最常见的一种本地化表示方法是将单词变为独热编码，但是方法最大的问题是它无法捕获单词之间的相似性或者关联性，同时独热表示会导致维数灾难，当词汇表非常大时，独热向量将会非常稀疏和高维

![CS224N_L1_19](.\assets\CS224N_L1_19.png)

所以在现代深度学习中，会使用向量来编码词汇中的相似性，这种方式依赖于**分布式语义**的思想，这种思想的核心就是一个单词的语义取决于上下文，而不是直接指定其语义，当然这种表示语义的方法不一定就是语义的最终理论，并且存在争议，但是这是一种计算性很好的语义表示方法，并且在深度学习中取得很大成功

![CS224N_L1_21](.\assets\CS224N_L1_21.png)

当一个单词出现的时候，就会带有一个上下文，就是你能看到的一系列词汇，比如说上图中的"banking"这个词汇，我们看到后会在文本中找到这个词汇出现的一堆地方，并且收集单词上下文中附近单词，这些词汇在某种意义上代表了"banking"这个词汇的含义

那么我们怎么将词汇变成一个向量呢？如下图所示，我们将上下文中出现过的单词视为向量，并且希望为每个单词构建一个稠密实数向量，这样就可以在某种意义上表示了这个单词的含义，然后就可以用来预测上下文中出现的其他词汇

当然，这里为了方便表示，向量只有八个维度，实际上常用的向量大小为三百维

这样就构成了一个分布式向量了，或者说分布式表示，因为词汇的语义分布在三百个不同维度上，这也被称为**词嵌入（Word Embeddings）**，实际上将词汇全部放置在高维向量空间中，或者说被嵌入到该空间中（下图所示）

![CS224N_L1_22](.\assets\CS224N_L1_22.png)

同时，如下图所示，分布式语义可以得知单词的相似性，并且将相似的单词组合到一起

![CS224N_L1_23](.\assets\CS224N_L1_23.png)

## Word2Vec算法

这是一个学习词向量的框架，13年被提出，目标就是基于一个语料库找出单词的良好向量（或者说学习出）

它的想法是这样的，假设我们有一个来自某个地方的文本，通常称为语料库（或者说文本正文），因此我们选择一个固定的词汇表，这个词汇表的长度就是语料库中不同单词的总数。例如，如果你有一个包含10000个不同单词的语料库，那么词汇表的长度就是10000。

然而，有时候，我们可能会选择设置一个最大词汇表大小，超过这个大小的词就被认为是未知词，或者被忽略。这主要是为了处理计算能力和存储资源的限制。对于非常大的语料库，其词汇表可能包含数百万甚至数十亿的唯一单词，这在计算和存储方面可能会带来很大的挑战。为了解决这个问题，我们可以设置一个词汇表的最大尺寸，并且只保留最频繁出现的单词。

1. **预处理**: 在训练模型之前，首先需要对输入的文本进行预处理。这可能包括去除停用词（如"和"、"的"、"是"等），进行词干提取或词形还原，或者对文本进行分词等操作。
2. **构建词汇表**: 通过预处理的文本，构建一个词汇表，其中每个唯一的单词都会分配一个唯一的索引。这些索引将用于为每个单词创建其对应的词向量。
3. **初始化向量**: 对于词汇表中的每个单词，都初始化一个向量。这些向量的维数是一个超参数，可以在训练开始前设定。初始向量的值通常是随机的。
4. **训练**: 根据选择的模型（CBOW或Skip-gram），在训练数据中遍历每个单词及其上下文，并使用反向传播和梯度下降的方法更新词向量。在CBOW中，输入是上下文词的向量，输出是目标词；在Skip-gram中，输入是目标词，输出是上下文词。在任何时候都有一个中心词$c$和上下文词汇$o$（意为outside）
5. **调整词向量**: 在每次迭代中，根据模型的预测错误程度，使用梯度下降来逐渐调整每个词的向量值。这个过程持续进行，直到模型的预测性能满足某个预设的标准，或者达到预定的迭代次数。一般来说，训练的目标就是最大化上下文词汇出现的概率
6. **获取词向量**: 训练完成后，每个词的词向量就是我们想要的结果。这些词向量可以用于各种NLP任务，比如文本分类、情感分析、机器翻译等。

最终，每个词都会有一个向量表示，而且这个向量能够捕捉到一些语义信息，比如相似的词在向量空间中会靠得比较近。这就是Word2Vec的主要目标。

其中，算法的核心就是objective function，这是一种类似于滑动窗口法的算法，在给定中心词$w_j$的情况下，预测固定大小$m$的窗口内的上下文词汇，然后使用极大似然估计和梯度下降法来最大化

![CS224N_L1_27](.\assets\CS224N_L1_27.png)

当然，这里实际上给每个单词分配了两个向量，一个是作为中心词的向量，一个是作为上下文的向量，这样更容易构建词向量系统

![CS224N_L1_28](.\assets\CS224N_L1_28.png)

在预测的时候，我们就可以直接通过向量之间的点积计算词汇之间的相似性，以及通过上下文推断中心词或者通过中心词推断上下文

![CS224N_L1_30](.\assets\CS224N_L1_30.png)

下一步就是转化为概率分布，这里也是使用softmax函数

至于训练模型的方法，就是梯度下降法

![CS224N_L1_31](.\assets\CS224N_L1_31.png)

当然，这个算法并没有使用词汇的位置信息，同时存在一些其他的问题

1. **无法处理词义消歧**：Word2Vec无法对多义词进行恰当的表示。一个词在不同的上下文中可能具有完全不同的含义，但在Word2Vec中，每个词只有一个向量表示，这使得模型无法区分同一个词在不同上下文中的不同含义。
2. **无法动态更新**：一旦训练了Word2Vec模型，就无法再向模型中添加新的词汇，除非重新进行训练。这意味着如果有新的词汇或者新的语料库出现，需要重新训练整个模型，而不能仅仅更新新出现的部分。
3. **忽略了词序**：Word2Vec的训练过程忽略了词的顺序，只关注了词的上下文。这使得它无法捕捉到一些词序相关的语义信息，例如否定和疑问等。
4. **需要大量的数据**：Word2Vec通常需要大量的训练数据才能得到良好的结果。对于一些数据量较小的情况，Word2Vec可能无法得到满意的词向量。
5. **词汇表外的问题**：对于词汇表外（out-of-vocabulary，OOV）的词，Word2Vec无法提供有效的词向量。虽然有些方法（如使用一个特殊的"UNK"标记）可以缓解这个问题，但并不能完全解决。

由于这些问题，后续有很多研究者提出了新的模型和方法，例如GloVe、FastText和BERT等，以解决Word2Vec的这些问题和局限性。

## gensim

Gensim是一个开源的Python库，它能够使用少量的内存高效地处理大型文本数据。它被广泛用于自然语言处理（NLP）和机器学习领域，尤其擅长主题建模和文档相似度分析。

以下是Gensim库的一些主要功能：

1. **主题建模**：Gensim提供了许多实现主题建模的工具，包括Latent Semantic Analysis（LSA/LSI）、Latent Dirichlet Allocation（LDA）和随机投影等。
2. **词向量训练和处理**：Gensim可以轻松处理Word2Vec、FastText和Doc2Vec等模型的训练和加载。这些模型都是基于神经网络的，能够生成词语或者文档的向量表示。
3. **文档相似度分析**：Gensim可以计算和比较文档之间的相似度，尤其是在主题建模和词向量训练的基础上。
4. **处理大型文本数据**：Gensim支持在线学习和处理大规模数据，这使得它非常适合处理那些无法一次性装入内存的大型数据集。
5. **可解释性**：在主题建模方面，Gensim生成的模型可以显示每个主题的关键词，以及各个词的权重，这有助于理解和解释模型的结果。

Gensim的API设计得清晰且富有一致性，这使得它在Python的NLP库中非常受欢迎。同时，Gensim还与其他的科学计算库（如NumPy、SciPy和Scikit-learn）进行了良好的集成，可以方便地在这些库之间进行数据的传递和转换。

```python
model=gensim.downloader.load("text")#导入文件
model['word']#类似于numpy的方式读取词向量
model.most_similar('word')#查看与某个词汇最相似的词，比如说与USA最相似的可能是Canada等词汇
```

当然，还有一些神奇的操作，比如说在这个向量空间中进行运算，比如说King减去Man加上Woman，得到的结果与Queen非常相似

![1689516267527](.\assets\1689516267527.png)

此外，我们还可以查找差异最大的词，当然这个功能用到的可能性不大

```python
model.mose_similar(negative='banana')#查找差异最大的词汇
model.mose_similar(positive=['woman','king'],negative=['banana'])#上图中计算的情况，可以查看这种运算之后最相似的词汇
```

