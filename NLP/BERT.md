# BERT：双向编码

双向编码，使用没有标号的数据来进行训练，这是一种无监督学习

## Related Work

在 BERT 之前，NLP 有两种主流的方法来进行预训练的特征表示，也就是 feature-based 和 fine-tune，前者有 ELMo，会基于特定下游任务来构建一个神经网络，然后在预训练的特征表示上进行应用，因为特征已经有了比较好的表示，所以模型训练起来比较容易，其中预训练的特征表示有词嵌入模型；然后就是 GPT 这种，在面对下游任务的时候不需要改动太多，只需要改动一点点既可以了。这两种方法在训练的时候都是使用同一个损失函数，并且都是一个单向的模型

这两种技术的一个问题就是，使用的语言模型都是单向的，比如说 GPT 就是一个从左到右的架构，只能从左往右看一个句子，但是在做一些句子级别的任务的时候就会有所不足，比如说判断一个句子的情感的时候，从左到右和从右到左都可以

## Basic

BERT 的架构解除了这种单向的限制，其使用了 Masked Language Model，也就是带有掩码的语言模型，这个模型会随机把一些单词遮盖，然后让模型预测这些单词，就类似做完形填空一样，这种架构允许模型双向的看句子

而且这种实际上是一种无监督学习，不需要标注大量的文本来训练

这种架构实际上就是利用了 ELMo 的双向 RNN 架构和 GPT 的Transformer 架构的长处并且把两者结合起来

## Framework

实际上分为两步，一步是预训练，一步是微调，其中预训练就是在无标号的数据上进行无监督训练，并且训练的文本数据可以覆盖不同的预训练任务，然后就是微调，在微调中，基于预训练的权重在特定的有标签的数据集上进行微调

实际上 BERT 就是一个多层双向的 Transformer 编码器

## Code

BERT 的输入可以分为三部分，Token Embedding，Segment Embedding 和 Position Embedding

其中

# GPT系列

按照发表时间的前后，是 Transformer - GPT - BERT - GPT2 - GPT3

## GPT1

在当时，语言处理的任务有很多，但是有标签的文本数据却没有太多，想在这些文本上训练判别模型是有困难的，所以作者的方法就是在大量的无标签文本上训练预训练的模型，然后在特定数据集上训练判别模型，这种方法实际上在视觉领域应用广泛，但是在语言领域却没有 ImageNet 那么庞大的有标签数据集，所以