# Vision Transformer

简称ViT，实际上是注意力机制在计算机视觉中的应用，或者说带有注意力机制的计算机视觉方法

因为注意力机制实际上是一种自适应的卷积核，而CNN是固定大小的卷积核，所以注意力机制下的计算机视觉实际上就是带有了自适应感受野的卷积网络，搜索空间更大，提高了准确度的上限，但是带来了训练成本更高的问题（要在更大范围内搜索最优解），导致现在工程落地中，很少使用ViT方案，更多地使用传统CNN方案

# SENet

Squeeze-and-Excitation Networks，简称SENet，主要的创新是引入了注意力机制并且实现了通道之间关联性的学习，在传统的卷积神经网络（CNN）结构中，不同通道之间的特征信息是独立处理的，没有考虑到通道之间的相关性。SENet引入了一种注意力机制，使得网络可以动态地对不同通道的特征进行加权，从而更加关注重要的特征信息，抑制不重要的特征，这种特性使得网络具备更强的特征表示能力。

![SENet_1](.\assets\SENet_1.png)

SENet的基本模块称为SE block，一个模块会考虑通道之间的关系并且进行学习，这个模块不仅可以用来堆叠并且构成一个SENet，还可以任意嵌入其他的网络

## Squeeze步骤

在这一步中，SENet通过全局平均池化（global average pooling）来对每个通道的特征图进行压缩（这也是Squeeze的本意）。这个过程将特征图转换为一个单一的数值，即通道的全局特征。这样做的目的是为了摄取每个通道的全局统计信息，以便后续的注意力机制对各个通道的重要性进行评估，实现了对通道依赖关系的利用，或者说在输出部分可以利用区域以外的上下文信息。

当然，可以使用其他的复杂聚合方法来实现，全局平均池化是其中一种

## Excitation步骤

在获取了各个通道的全局特征之后，就是进行Excitation（激励）步骤了，这是一个自适应重标定的过程，可以实现对前一步中汇聚的信息的利用，在这一步中，SENet利用一个或者多个小型的多层感知机（MLP）来学习对应于每个通道的重要性权重。该MLP接收来自Squeeze步骤得到的全局特征作为输入，并输出一个通道注意力向量。这个通道注意力向量通过对每个通道进行逐元素乘法，来动态地对特征图的每个通道进行加权

## 应用

我们可以将SE模块和现代CNN架构结合，然后实现一些功能

我们使用Inception与SE结合，得到

![SENet_2](.\assets\SENet_2.png)

与ResNet结合得到

![SENet_3](.\assets\SENet_3.png)