# Vision Transformer

简称ViT，实际上是注意力机制在计算机视觉中的应用，或者说带有注意力机制的计算机视觉方法

因为注意力机制实际上是一种自适应的卷积核，而CNN是固定大小的卷积核，所以注意力机制下的计算机视觉实际上就是带有了自适应感受野的卷积网络，搜索空间更大，提高了准确度的上限，但是带来了训练成本更高的问题（要在更大范围内搜索最优解），导致现在工程落地中，很少使用ViT方案，更多地使用传统CNN方案

# SENet

Squeeze-and-Excitation Networks，简称SENet，主要的创新是引入了注意力机制并且实现了通道之间关联性的学习，在传统的卷积神经网络（CNN）结构中，不同通道之间的特征信息是独立处理的，没有考虑到通道之间的相关性。SENet引入了一种注意力机制，使得网络可以动态地对不同通道的特征进行加权，从而更加关注重要的特征信息，抑制不重要的特征，这种特性使得网络具备更强的特征表示能力。

![SENet_1](.\assets\SENet_1.png)

SENet的基本模块称为SE block，一个模块会考虑通道之间的关系并且进行学习，这个模块不仅可以用来堆叠并且构成一个SENet，还可以任意嵌入其他的网络

## Squeeze步骤

在这一步中，SENet通过全局平均池化（global average pooling）来对每个通道的特征图进行压缩（这也是Squeeze的本意）。这个过程将特征图转换为一个单一的数值，即通道的全局特征。这样做的目的是为了摄取每个通道的全局统计信息，以便后续的注意力机制对各个通道的重要性进行评估，实现了对通道依赖关系的利用，或者说在输出部分可以利用区域以外的上下文信息。

当然，可以使用其他的复杂聚合方法来实现，全局平均池化是其中一种

## Excitation步骤

在获取了各个通道的全局特征之后，就是进行Excitation（激励）步骤了，这是一个自适应重标定的过程，可以实现对前一步中汇聚的信息的利用，在这一步中，SENet利用一个或者多个小型的多层感知机（MLP）来学习对应于每个通道的重要性权重。该MLP接收来自Squeeze步骤得到的全局特征作为输入，并输出一个通道注意力向量。这个通道注意力向量通过对每个通道进行逐元素乘法，来动态地对特征图的每个通道进行加权

## 应用

我们可以将SE模块和现代CNN架构结合，然后实现一些功能

我们使用Inception与SE结合，得到

![SENet_2](.\assets\SENet_2.png)

与ResNet结合得到

![SENet_3](.\assets\SENet_3.png)

# Non-local Neural Network

这是CVPR2018的论文，是一篇很核心的文章

当时的CNN和RNN都是一种局部的操作（Local operations），都是受到图像处理中的非局部均值方法启发，提出了非局部操作的方法，或者说是长距离依赖的想法，也就是说在计算的时候会考虑远处的信息，而且不一定是在同一个维度上的，比如说在图像上会考虑像素与像素之间的关系，在视频中会考虑不同时间中不同位置区域的关系，在语言中会考虑不同词汇之间的关系，而且这个操作可以插入到很多网络中去

在CNN或者RNN中，长距离的依赖关系，只能通过多层卷积操作堆叠带来的大感受野来进行计算，或者说CNN和RNN中的卷积与循环操作，都是一种局部关系的计算方法，需要重复多次局部计算才可以去计算长距离依赖，但是这种重复性的局部计算是效率很低的，并且会导致训练上的困难，还有信息传递困难的情况，非局部操作就可以有效解决这些问题，并且可以通过更少的层来实现很好的效果，此外，非局部操作也是注意力机制的一种实现，可以接受任意大小的输入并且可以与其他操作进行组合

![Non_local_Neural_Network_1](.\assets\Non_local_Neural_Network_1.png)

比如说在这个视频分类问题中，在时间和空间维度上进行了非局部操作，比如说这个球的位置$X_i$就是由所有位置计算得到的，其中权重高的一些显示在上图中

在此方法中，基本的模块为non-local block，

此外，non local block比三维卷积在计算上更经济