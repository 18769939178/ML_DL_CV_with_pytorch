# CNN架构

上一节课我们谈到了很多卷积神经网络的部分，但是仍然有一个问题，我们并不知道如何将这些部分组建成一个完整的神经网络来识别图像

这节课的内容就是构建了一整个卷积神经网络来实现图像分类，并且介绍了CNN发展的历史（2012-2019）并且分析了不同的CNN架构

## CNN历史：ImageNet挑战赛的发展

## AlexNet

AlexNet是计算机视觉领域历史性的论文，或者说是深度学习计算机视觉的开创性论文，代表了计算机视觉或计算机科学领域的重要进步

AlexNet的架构如下

![](./assets/AlexNet-Fig_03.png)

第一层是卷积层，卷积核大小为11x11，数量为64，步长为4，填充为2，所以输出大小为64通道，宽高为56
![](./assets/eecs8-28.jpg)

![](./assets/eecs8-31.jpg)
Alexnet 早期卷积层需要较多的内存开销，且相比全连接层需要更多的浮点数计算；全连接层占用的参数远大于卷积层，且展开后的第一个全连接层需要的参数最多。
## ZF Net
![](./assets/eecs9-33.png)

这个网络实际上就是一个更大的AlexNet，并没有做出太大的创新，只是在某些超参数上进行了调整，思想上与AlexNet一致

ZFNet的第一层使用两倍的下采样因子（步长为2），相比于AlexNet有更大的空间分辨率，也就意味着有更大计算量

从AlexNet到ZFNet的启示就是，更大的网络可以更好的工作，只不过这时候并没有方法可以使得网络变换大小

## VGG网络

VGG网络带来的启示就是，没必要直接使用更大的卷积核，可以使用多个更小的卷积核来代替，比如说一个5x5的卷积核，就可以使用两次3x3卷积来代替

VGG的设计比较标准，或者说架构设计上比较有原则

其设计原则是，所有卷积层的卷积核大小都为3x3步幅为1填充为1，所有池化层的都是2x2的最大池化层且步幅为2，池化层之后会将通道数量加倍

VGG不是五个卷积层，而是五个阶段，每个阶段都是几个卷积层和一个池化层，后面就是全连接层

或许可以在两个卷积层之间添加ReLU层来提供非线性计算，来提高其拟合能力

## GoogleLeNet

GoogleLeNet是谷歌团队研究出的网络，发表于2014年，这是谷歌团队为了致敬Yann LeCun及其创建的LeNet所命名的，其特点之一是尝试更高效的神经网络，降低其复杂性，使得可以在手机上运行

![](./assets/eecs8-52.jpg)
另一个特点就是使用Inception Module来代替传统卷积，这个模块是一种重复的局部结构，由四路并行的卷积/池化模块组成,并在最终使用全局平均池化层来折叠空间维度。

但是，GoogleLeNet并没有使用批量归一化方法，或者说这时候并没有批量归一化方法，所以这个网络使用的是其他方法来更高效的处理，即输出三种不同的类分数，因为传播通过三个分类器返回的梯度，可以使得反向传播更容易，使得模型更容易收敛

但是在批量归一化出现之前，想训练一个大型的网络是相当困难的，所以有一种解决方法，就是先训练一个浅层网络，然后在其收敛之后插入新的层，继续训练

## ResNet

2015年ResNet出现，这是具有划时代的意义，因为批量归一化的方法出现（这是一个很有意义的创新），然后神经网络的深度可以大大加深，一年内网络深度从22层增加到152层，在这之前，更深的网络表现的可能比浅层网络更差

## MobileNet

这是一个非常轻量化的网络，可能其准确率并不会高过那些网络，但是轻量化的结构可以使得在计算的时候非常快速，并且在手机登设备上实现这些计算

# 神经网络架构研究

设计一个神经网络架构是困难的，所以需要自动化实现

我们有一个称为控制器的神经网络，这个神经网络将输出另一个神经网络，所以训练过程是我们采取