# 前文总结

之前已经讲了包括CNN结构、训练方法等，但是这些实际上只是一种前馈结构，所有的信息都是前向传播（有分支），每一层都接受单一输入（如图像等），每一层的输出都会进入下一层进行处理，最后输出一个单项，比如说一个完整的神经网络，我们输入一个图像，就可以输出一个标签（尽管这个标签未必是对的）

但是我们想要使用深度学习神经网络解决其他问题的话，CNN就难以解决了，比如说我们的输入不是一个，或者输出不是一个的情况（例如，输入一个序列，输出一个序列这样子）

此外，序列长度是不固定的，所以我们需要一种新方法来处理，这就是循环神经网络（Recurrent Neural Networks，RNN），实际上这种网络对于处理非顺序数据也非常有用（有人使用CNN）

## MNIST的RNN处理示例

有人使用RNN，对MNIST数据集进行分类，不过其分类的方法不同于CNN，CNN是整张图像输入，RNN则是大量的“glimpses”，可以理解为每次看到图片的一小部分（大小不一定相等），看到多次之后进行分类

## 图像生成

RNN可以实现图像生成，输入一连串指令，然后RNN就可以生成一系列图像

# 循环神经网络

## 结构

CNN只有三部分，内部固定参数、输入、输出，即输入X，经过内部参数之后得到Y输出

但是RNN则不然，多了一个内部隐藏状态（或者叫内部状态），每一次输入之后，都会使用某种公式来更新这个状态，可以使用一个递归公式给出

![16](./assets/16-1683192701565-16.jpg)

## 示例：Vanilla RNN

这是一个简单的循环神经网络

![17](./assets/17-1683192919272-18.jpg)

## RNN计算图

我们以计算图的方式思考RNN是怎么工作的

![22](./assets/22-1683193115863-20.jpg)

我们首先给定一个初始状态$h_0$，然后不断在输入中进行更新，每一次我们都将状态和输入提供给递归关系函数，然后输出第一个隐藏状态，依次递归

注意一下，序列每次运算都是相同的矩阵，或者说在序列的每个时间步使用完全相同的权重矩阵，所以你可以看到在计算图中这表现为有一个不变节点W