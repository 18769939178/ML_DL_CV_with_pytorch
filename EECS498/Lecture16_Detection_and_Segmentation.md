# 回顾目标检测

## 历史发展

计算机视觉的历史可以追溯到很久之前

在深度学习元年（2012年）之前，所有的计算机视觉任务的精度都不是很高，发展平稳，而且在发展到一段时间后陷入了瓶颈期（2010-2012的mAP指数上升缓慢）

![4](./assets/4.jpg)

但是在2013年开始，人们将深度学习应用于计算机视觉任务中，使得视觉的精度有了快速的增长，而且还是持续的增长

当然上面的测试是在VOC数据集上测试的，这是一种相对简单的数据集，所以在2016年开始，人们转而去处理更具挑战性的数据集，大多数方法甚至不再费心测试这个数据集

## 回顾目标检测任务

在第一个经典目标检测算法R-CNN中，使用训练好的CNN来对那些使用选择性搜索方法筛选出来的区域进行预测，然后尝试拟合出一个最好的边界框，因为最开始的方法是基于选择性搜索的，所以这种候选区域方法可以视为黑盒

![5](./assets/5-1684214226845-57.jpg)

然后，我们使用先卷积提取特征然后提取候选区域，这种方法大大提高了目标检测的效率；后来，神经网络方法替代了选择性搜索，目标检测速度也再度提高

## R-CNN系列模型的训练方法

在上一节里面，老师并没有讲到R-CNN应该如何完成训练，所以这里对训练方法进行讲解

### Slow R-CNN

在Slow R-CNN（第一代R-CNN的戏称）中，先接受一个彩色图像，并且可以访问到其中的真实边界框和标签，然后在输入图像上运行候选区域法，筛选出来一些候选区域

不过注意一下，选择性搜索是一个固定的算法，而非学习的部分。

选择性搜索是一种基于贪心策略的区域提议算法。它首先在多种尺度和颜色空间下对图像进行分割，然后根据颜色、纹理、大小和形状兼容性来合并相似的区域，生成一系列的候选区域。这个过程并不涉及学习，也就是说，它不会根据训练数据来调整自己的行为。

然后，在训练的时候，还有一个重要步骤，就是使用第一个CNN对候选区域进行分类，让其区分背景和对象，可以通过与真实边界框与标签进行对比来完成这一点

如下图所示，GT边界框表示真实边界框，包括两只狗和一只猫，其他颜色的是所有候选区域，可以看到，有的候选区域与GT边界框很吻合（深蓝色边界框），有的只包括对象的一部分（浅蓝色边界框），有的则只是背景（红色边界框）

![9](./assets/9-1684216491637-1.jpg)

我们知道在Slow R-CNN中我们会使用一个卷积网络对候选区域进行分类，分为正的目标类和负的背景类，当然，在这里我们还有一种类，就如上图中的浅蓝色框，只包含了对象的一部分，虽然也不是错误的，但是离完整的边界框还有一段距离，可以称为中性框

至于如何区分，可以使用交并比来判断，比如说一个区域与正边界框的交并比小于0.3，就可以认为这是负边界框，这里的交并比阈值也是一个超参数

这样，我们就得到了一些符合实际情况的边界框，比如说绝对不包括对象的负边界框，基本包括对象的正边界框还有位于中间状态的中性框

但是我们在训练的时候会忽略中性框，因为在这上面训练网络的话可能会产生混淆，中性框不是我们需要的也不是我们想排除的，所以在训练的时候直接忽视

下一步，挑选出来正边界框或者正类候选区域，将裁剪出的图像进行大小调整，转化为标准分辨率的图像，然后使用另一个卷积神经网络去预测类别和边界框回归变换

![11](./assets/11-1684221088780-3.jpg)

也就是说，对于负类，我们只需要区分类别就可以，对于正类，我们再进行分类和定位，所以需要让二分类网络记住这些背景区域和目标区域

### Fast R-CNN

Fast R-CNN的训练方法与Slow R-CNN的方法基本一致，只不过因为在Fast R-CNN中是先卷积来提取特征，然后使用候选区域法裁剪特征，这样就减少了大量重复计算，加快了效率

## 特征裁剪

不过在Fast R-CNN中，会使用一种特征裁剪的方式RoI池化，来对区域进行投影，投影到特征图上，这样就可以简化计算，然后对区域对应的特征图进行池化，然后输入到第二阶段中去，这样可以便于对齐，可以更好的进行目标检测

![20](./assets/20.jpg)

我们实现的方法就是使用双线性插值，在图像的特征图的单元直接进行插值

![23](./assets/23-1684225827454-6.jpg)

这种方式还可以解决反向传播问题

## 不依赖锚框的目标检测方法

我们知道，Faster R-CNN和单阶段目标检测方法是依赖于锚框的，所以有没有不依赖于锚框的检测方法呢

![30](./assets/30.jpg)

实际上是有的，那就是密歇根大学的几个教授在2018年完成的这篇论文，其中的创新点是使用CornerNet来实现目标检测，不同于之前的方法

![31](./assets/31-1684227508702-9.jpg)

这个网络的想法是，改变我们对边界框的参数化方式，我们现在要用左上角和右下角来表示边界框，然后用来检测边界框，我们只需要让让图像的像素决定

1. **无需候选框生成**：Faster R-CNN 使用了区域提议网络（Region Proposal Network，RPN）来生成候选框。相比之下，CornerNet 直接在图像中预测目标的两个角（左上角和右下角），无需生成和处理大量的候选框。这种方法避免了候选框生成的计算复杂性和可能的错误，提高了效率。
2. **基于关键点的检测方式**：CornerNet 使用的是一种基于关键点的目标检测方法，它预测的是目标的角点，而不是常见的边界框。这种方式在理论上可以更精确地定位目标，因为角点的位置可以是任意的，而不仅仅是像边界框那样的轴对齐的矩形。
3. **单阶段检测**：CornerNet 是一种单阶段的目标检测方法，即它直接在输入图像上预测目标的位置和类别，无需像 Faster R-CNN 那样分成两个阶段进行。这也使得 CornerNet 在速度上具有优势。
4. **损失函数的改进**：CornerNet 使用了一种新的损失函数，即 focal loss 和 corner pooling，这在一定程度上解决了类别不平衡问题，并改进了角点的预测性能。

# 语义分割（Semantic Segmentation）

语义分割任务就是我们对图像中的每个像素打标签，比如说下图中这只猫，我们希望对属于猫这个对象的每个像素都打上相应的标签

![34](./assets/34-1684230376740-11.jpg)

当然，在语义分割中，并不会区分不同的对象，只是负责给对象打类别标签，也就是说，如果有两个紧邻的同一个类别的对象，那么会打上同一种颜色的标签（如上图中的奶牛），不会区分不同的实例

## 初步：滑动窗口法

我们有一个不成熟的想法，就是使用滑动窗口的方法，不断预测某一个小区域的类别，然后给予相应的标签

![36](./assets/36-1684231681428-13.jpg)

当然这种方法效率是很低的，因为需要大量重复的计算，并且没有共享特征

实际上我们绝对不会在实践中使用这种方法，但是我们可以从中获得一些启发

## 全连接卷积网络

我们使用一种名为**全连接卷积网络（Fully Convolutional Network）**的网络架构来实现语义分割任务，这是一个没有全连接层和全局池化层的网络，它只有一堆卷积层，输出与输入有同样的二维形状（或者说输出形状由输入形状决定），输出是每个图像的分类分数，或者说具有与类别数一样的通道数的三维张量，然后使用最大化函数就可以得到一个我们想要的语义分割图

然后损失函数则是每个像素的交叉熵损失

![37](./assets/37-1684232291206-15.jpg)

当然，模型如何知道图像中有多少类别呢？其实这个类似于分类网络，我们预先选择一组类别（这个由训练数据集决定），然后进行设置

为了做出好的分割决策，我们实际上可能希望根据输入图像中相对较大的区域来做出决定，所以我们想，如果使用3x3卷积，那么感受野大小会是线性增长的，两个3x3卷积叠在一起，那么第二层的输出实际上就是查看5x5的区域，每多一层，感受野大小就会加二，这意味着我们实际上需要非常多的层才能获得非常大的感受野大小

![39](./assets/39-1684233836073-17.jpg)

所以就会产生问题，我们经常想要在相对高分辨率的图像上完成分割，比如说互联网图像和卫星图像，这些图像可能有以百万计的像素，导致这种方式的计算量非常大，所以需要进行改进

实际上，人们会使用上采样和下采样的架构来完成语义分割

## 带有上/下采样的全连接卷积网络

首先解释一下什么是上采样和下采样

**下采样（Downsampling）**：下采样是一种降低数据的分辨率或者复杂度的处理方式。在图像处理中，下采样通常是指减少图像的像素数量，使图像变得更小、更模糊。在深度学习中，下采样通常通过使用卷积层（特别是步长大于1的卷积层）或池化层（如最大池化或平均池化）来实现。通过下采样，我们可以减少计算的复杂度，同时提取出图像的高层次（也就是更抽象的）特征。然而，下采样的过程会丢失一些细节信息，这对于需要精确像素级别预测的语义分割任务来说，可能是一个问题。

**上采样（Upsampling）**：上采样是一种增加数据分辨率或者复杂度的处理方式。在图像处理中，上采样通常指增加图像的像素数量，使图像变得更大、更清晰。在深度学习中，上采样通常通过使用转置卷积（也叫反卷积）或者插值（如最近邻插值或双线性插值）来实现。上采样的目标是恢复下采样过程中丢失的细节信息，从而使我们可以在高分辨率的输出上进行预测。这对于语义分割任务来说，是非常重要的。

在网络开始的时候进行下采样，这样就可以提取很多特征，就类似于图像分类中一样，我们甚至可以使用池化方式去进行下采样，但是什么是上采样呢

![41](./assets/41-1684236405827-19.jpg)

上采样可能是一种与池化操作相反的过程，或者可以称为**反池化（Unpooling）**

比如说我们输入一个2x2大小的矩阵，上采样就可以实现一个空间上两倍大小的输出，通道数不变

![43](./assets/43-1684236587614-21.jpg)

**钉床（Bed of Nails，也可译为床钉）**：这种方法的名字来源于它的工作原理与形状。"Bed of Nails"是一种上采样策略，其工作原理类似于在下采样时丢失的位置放置"钉子"（即非零值），然后通过插值或其他方法在其他位置生成值，从而恢复原始的分辨率。在这里，我们是将原特征向量复制到每个对应区域的左上角，然后其他办法用零值填充。当然这不是一个很好的方法，所以人们通常使用另一种方法。

**最近邻反池化（Nearest Neighbor）**：最近邻上采样方法的基本思想是：对于每一个在上采样后的新的像素位置，选取最近的原始像素值作为其值。因为它直接使用最近的像素值，所以这种方法相对简单快速，但可能会在图像中引入一些锯齿状的效果（因为它不会像一些其他上采样方法那样进行平滑处理）。

在实践中，最近邻上采样常常被用在一些需要快速但不需要过分关注图像质量的场景中。在深度学习的语义分割任务中，尤其是在进行分辨率恢复（即上采样操作）的时候，最近邻上采样也是一个常见的选择。

**双线性插值（Bilinear Interpolation）**：在图像上采样中，双线性插值的操作可以这样理解：对于目标图像中的每一个像素，先在水平方向进行线性插值，然后在垂直方向进行线性插值，得到最终的像素值。

![44](./assets/44.jpg)

举个例子，如果我们想在一个2x2的像素格子中插入一个新的像素，我们可以先在水平方向上，对左侧像素和右侧像素进行线性插值，然后在垂直方向上，对上侧像素和下侧像素进行线性插值，最后，我们就得到了新的像素的值。

相比于最近邻插值，双线性插值能够得到更加平滑的图像，因为它在插值过程中，考虑了像素之间的关系，能够更好地保留图像的细节信息。但同时，双线性插值的计算量也比最近邻插值要大一些。

**双三次插值**：它是一种更高级的插值方式，相比于最近邻插值和双线性插值，可以得到更加平滑、更少锯齿状的图像。

![45](./assets/45.jpg)

在双三次插值中，新的像素值是通过考察附近16个像素（在一个4x4的区域内）并使用这些像素的值进行加权计算得到的。具体的加权系数由三次插值函数决定，这个插值函数考虑到了像素之间的距离关系。

由于双三次插值考虑了更多的邻近像素，并且使用了更为复杂的插值函数，所以它能够得到更加精细、更为平滑的图像。但是，双三次插值的计算复杂度也较高，需要更多的计算资源。

**最大反池化（Max Unpooling）**：这种方法的主要目的是恢复被池化（pooling）操作降低的图像分辨率。

![46](./assets/46-1684244400435-2.jpg)

在最大池化（Max Pooling）操作中，输入的特征图被分成若干个非重叠的区域，每个区域内取最大值作为该区域的代表，这样能够大大减小特征图的尺寸。但在这个过程中，我们会丢失很多细节信息，这对于语义分割任务来说是不可接受的。

为了解决这个问题，我们可以使用最大反池化操作来恢复原来的图像尺寸。具体来说，最大反池化是这样进行的：首先，创建一个和原图像尺寸相同的空矩阵，然后，将池化操作中得到的最大值填充到对应的位置上，其他位置用零填充。这样，就能够得到一个和原图像尺寸相同的特征图。

值得注意的是，最大反池化操作需要记录最大池化操作中的最大值位置，这是因为我们需要在反池化过程中将最大值放回原来的位置。因此，最大反池化操作通常与最大池化操作配合使用。

**理解**

在语义分割任务中，下采样和上采样操作通常会配合使用，以形成一个编码-解码（Encoder-Decoder）的结构。这种结构也被称为 U-Net 结构，因为它的形状类似于字母"U"。

在编码阶段（即下采样阶段），网络通过连续的卷积层和池化层来提取输入图像的语义特征。这些操作可以将图像的空间维度（即高度和宽度）降低，同时提升图像的深度（即通道数），使得网络能够捕捉到更抽象的特征。然而，这个过程会导致图像的空间信息丢失，这对于语义分割任务来说是不可接受的。

为了恢复图像的空间信息，网络在解码阶段（即上采样阶段）使用一系列的反卷积操作或上采样操作，以将特征图的空间维度恢复到与原始输入图像相同的尺寸。在这个过程中，网络通常会将上采样后的特征图与对应的下采样阶段的特征图进行拼接或相加，这被称为跳跃连接（skip connection）。这种方法可以帮助网络更好地保留低级别的空间信息，从而提高分割的精度。

经验法则是，如果你如果在下采样部分使用平均池化这种，那么上采样就可以考虑最近邻或者双线性或者双三次插值，如果下采样是最大池化，那么应该考虑使用最大反池化作为上采样方式

## 转置卷积（Transposed Convolution）：可学习的上采样

上面这些上采样运算，并没有任何可学习的参数，都是固定的运算，但是这里有一种转置卷积的上采样方法，可以通过某种方法进行学习

普通的带有步长和填充的卷积大家很熟悉了已经（如下图所示），那么可学习的卷积是什么呢？

![49](./assets/49-1684246930314-4.jpg)

我们知道，步长大于等于1的卷积，实际上就是一个可学习的下采样操作，去提取特征，而可学习的这个属性意味着网络可以学习如何最好地进行下采样，以便最大化某些任务的性能，这也是为什么许多现代的卷积神经网络结构，如ResNet和DenseNet，倾向于使用步长大于1的卷积替代池化操作进行下采样。

但是，如果我们的步长小于1呢？这样，卷积可以通过某种方式，为输入中的每个点跨越输出中的多个点，如果我们能想出一种方法来做到这一点，那么是不是就可以实现一种可学习的上采样操作？实际上是有的，也就是**转置卷积**

![58](./assets/58.jpg)

它的工作方式是这样的，首先输入一个低分辨率的数据，想输出一个高分辨率的数据（如上图所示），不过与常规卷积不同，我们这里是使用输入张量中的元素乘以转置卷积核（标量乘以张量），然后得到输出，并且将数值复制到输出张量的相应位置上，然后我们在输入中移动一个位置，在输出中移动两个位置，再次执行反卷积操作，如果有重合的地方，则进行求和