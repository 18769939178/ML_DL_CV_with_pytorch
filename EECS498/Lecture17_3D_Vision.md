# 三维视觉概述

前面我们我们讨论了视觉中的不同任务，包括语义分割，目标检测，实例分割等等，我们还讨论了关键点估计的一些话题，这些算法在实际生活中有非常多的应用，当然，这些算法都是基于二维图像的

但是，我们的世界不是二维的，而是三维的，这是一个完全独立的研究领域，研究如何增加空间维度到我们的神经网络模型，所以今天的内容就是如何将三维信息传递到神经网络模型中

![4](./assets/4-1684306645077-1.jpg)

## 两类问题

我们这里要关注两类问题

第一个就是从单个图像预测三维形状的任务（如下图所示），我们输入一些彩色图像，然后输出该图像中对象的三维形状的表示

第二个就是形状预测的任务，输入一些三维形状数据，然后进行预测，比如说分类或者分割任务

![5](./assets/5-1684306823066-3.jpg)

这些任务都是基于监督学习的，所以我们有一个带有输入图像和对象三维形状的训练集（或者三维形状和对应标签的）

## 三维视觉中的更多主题

三维视觉的内容不仅仅是完成分类分割，实际上还有更多的相关主题（很多主题并不是基于深度学习方法的），这是因为三维视觉会涉及对象和世界的三维结构

![6](./assets/6-1684312373442-5.jpg)

我们可能有这样一直想法，就是我们想输入一个视频序列，这是一个二维图像帧序列，然后我们想通过视频，预测或者重建摄像机穿过三维世界的轨迹

但是在这里我们只涉及两部分，一个是有监督形状预测，一个是有监督形状分类

# 三维形状表示（3D Shape Representations）

首先，我们应该如何去表述三维形状呢？我们这里有五种表示方式，可以用来模拟三维形状和信息，这也是人们常用的类型，有不同的优缺点，每一种都可以用神经网络模型去预测和处理

下图中展示了五种表达方式，每一种都是相同底层三维形状的不同表示

![7](./assets/7-1684314920951-7.jpg)

## 深度图（Depth Map）

### 概念

深度图在概念上是一个非常简单的三维形状表示，深度图的作用是对于输入图像中的每个像素，分配从相机到该像素的距离，因为图像中的每个像素都对应于现实世界中的某个对象，现在深度图告诉我们，对于图像中的每个像素，相机与该像素试图表示的现实世界中的那个位置之间的距离是多少，单位是米。

![9](./assets/9-1684315242021-9.jpg)

不过深度图比以往的RGB图要多一个网格，其中每个像素值给了我们一个深度信息，有些类似于RGB图像的二维网格，只不过像素的值不是颜色，而是“深度”，以米为单位的深度，然后将其与RGB图像结合，深度作为第四个信息通道，提供深度信息

或者我们可以叫他为2.5D图像或者RGB-D图像，因为深度图不像一个真正的完整三维表示，因为它的缺点是不能正确捕捉被遮挡物体的结构，比如说上图中，书柜的一部分被沙发遮挡了，那么相应的深度图就没有沙发后面书柜的任何三维表示，所以说RGB-D只能表示图像的可见部分，出于这个原因，我们可能认为它是一种不是很强大或者不是很通用的三维表示

但是这种深度图类型的数据是很重要的，因为我们可以使用各种原始3D传感器去捕获深度图数据，比如说结构光相机

### 深度预测

不过我们可以想尝试一个任务，就是输入一个普通RGB图像，然后尝试逐像素地预测相机到对应物体的距离（如下图所示），使用全卷积网络完成这个深度预测任务

![10](./assets/10-1684320279577-1.jpg)

但是实际上这是不太可行的，因为在三维视觉中，会有**尺度/深度歧义（Scale / Depth Ambiguity）**的问题，比如说对于一张图片，无法区分出远处大物体和近处小物体，或者说仅通过单张二维图像，我们无法准确地确定物体的实际大小和距离

如下图所示，大猫是小猫的两倍大小，但是与相机的距离也是小猫的两倍，那么看起来是完全一样的大小

![11](./assets/11-1684322287115-3.jpg)

也就是说，绝对尺度和绝对深度在单个二维图像中并不明确，所以会产生这种问题，所以在处理任何类型的三维表示或者三维预测问题的时候，考虑这个潜在的问题是很有必要的

但是我们可以通过改变神经网络的结构来处理尺度/深度歧义问题，或者说我们可以通过改进损失函数来完成深度预测问题

![12](./assets/12-1684323118524-5.jpg)

因为尺度和深度实际上是不变的，假设我们的网络模型预测了一个深度，达到了Ground Truth Depth的某个恒定比例，那么上图中的尺度不变损失函数仍会将这个情况认定为零损失，或者说，这个尺度不变损失函数只会关心预测深度是不是匹配真实深度的某个固定倍数，这个属性细节叫做比例方差

### 表面法线（Surface Normals）

这是第二种三维形状表示，想法上与RGB-D深度图很接近，只不过在这里，我们给每个像素分配的是一个单位向量（或者说法线）

每个表面点的法线都是垂直于该表面的单位向量。对于平面，法线的方向是确定的，但对于曲面，每个点的法线可能都不同。在计算机图形学和计算机视觉中，表面法线被广泛用于光照计算，以判断物体表面的亮度和颜色。

理解表面法线也对物体识别和场景理解有重要作用。例如，通过分析一幅图像中的表面法线，我们可以推断出物体的形状和结构。在一些深度学习应用中，表面法线也被用作一种重要的特征来帮助模型更好地理解场景。

![13](./assets/13.jpg)

不过在这里，我们会使用RGB颜色来绘制法线的情况，比如说上图中，蓝紫色表示法向量是向上的，红色表示向前的

当然我们也可以使用一个网络来预测表面法线，我们可以输入RGB图像，然后去预测每个位置的三维向量

![14](/home/robot/Project/ML_DL_CV_with_pytorch/EECS498/assets/14-1684326806423-8.jpg)

然后我们的损失函数是对比两个法向量之间角度的差异，这里我们使用点积除以范数的方式进行归一化，以此来训练网络

或者可以训练一个联合网络，同时完成语义分割、深度估计和表面法线估计

这是一个相对简单的表示，但是在实践中是有用的，因为一旦有了深度图和表面法线，就可以得到很多关于图像三维结构的信息

当然，缺点还是不能表示被遮挡的地方

## 体素网格（Voxel Grid）

### 概念

"Voxel"是"volume"（体积）和"pixel"（像素）的结合词，指的是在三维空间中的像素，也就是体素

体素网格就像是三维的像素网格，它将三维空间划分成了一系列的小立方体（也就是体素），可以使用某些方法看到每个体素是否被占用（比如一个bool值），这有点像我的世界一样。每个体素都可以存储一些信息，例如颜色、密度、表面法线等，有点像Mask R-CNN中物体背景和前景的表示。通过这种方式，我们可以在计算机中表示和处理三维对象。

![16](./assets/16-1684327563074-10.jpg)

体素网格在许多领域都有应用，如计算机图形学、医学成像、地质科学和机器人导航等。例如，在计算机图形学中，体素网格可以用于渲染复杂的三维场景；在医学成像中，MRI和CT扫描的结果通常就是体素网格；在机器人导航中，体素网格可以用于表示环境并进行路径规划。

然而，体素网格也有一些缺点，例如它需要大量的存储空间，而且处理速度可能比较慢，尤其是在需要捕捉非常精细的细节的时候，分辨率回非常高，比如说图中的椅子就使用了非常多的像素。因此，在实际应用中，人们经常使用各种优化技术，如八叉树（Octree）和稀疏哈希表（Sparse Hashing），来高效地存储和处理体素网格。

### 处理：三维卷积

如果我们想对这些体素网格进行分类的话，一样可以使用卷积网络的架构，不过我们需要使用一种三维卷积的方式去完成

输入数据是一个原始的体素网格，每个点都有占用或者未占用两种情况，如何我们使用三维卷积核（类似于一个立方体）去滑动来计算内积，有点类似于一个立方体滑动着遍历这个空间的每个地方，然后产生下一层的标量输出，若干次卷积处理之后进入分类层，这在架构上与之前的二维卷积完全一致

![17](./assets/17-1684329699906-12.jpg)

当然，这里维度还是稍微有所不同的，每个阶段的张量都是一个四维张量，三个空间维度和一个特征维度（或者通道维度，表示体素是否被占用，这个是二进制的）

### 预测体素：三维卷积

我们下一个想完成的任务就是使用RGB图像去预测三维形状的体素网格

左边输入的是一个RGB图像，两个空间维度和一个RGB通道维度，我们需要预测一个四维张量，带有三个通道维度和一个通道维度，可以通过网格中每个点的占用概率信息，这需要我们设置一些架构，来添加额外的空间维度，然后使用交叉熵损失来训练这个网络

![18](./assets/18-1684330420049-14.jpg)

一种常见的完成这个任务的方式就是使用桥接的方式去连接不同维度的张量

首先我们对这个RGB图像使用卷积网络处理得到二维图像的特征图（这是一个三维张量，空间维度加一个特征维度），然后展开成一个特征向量，然后使用一个全连接层将其重塑为四维张量，然后我们就可以使用上采样方式得到三维表示

当然，这种方式的计算成本很大，所以并不适合

### 预测体素：体素管

"Voxel Tubes"是一种从二维图像生成三维体素网格的方法。这个方法基于一个假设，即每个像素的颜色和亮度都对应于一个小的、垂直于图像平面的体素管（Voxel Tube），这些体素管可以被组合在一起，形成一个完整的三维体素网格。这种方法只基于二维卷积，在计算上更有用

首先我们输入一张RGB图像，然后使用二维卷积得到一个CxHxW的特征图，但是在网络的最后一层非常特殊，因为我们想预测体素的输出，那么就要在这里安排空间卷积

不过，在这里实际上使用两个空间维度和一个通道维度，但是在计算损失的时候，我们会解释通道维度为输出张量的深度维度

在卷积的最后一层，就沿着通道维度去预测一个Tube，给我们预测一整管的体素概率

![19](./assets/19.jpg)

在这个方法中，每个体素管的高度（也就是在深度方向上的大小）可以由图像的亮度决定，而宽度和长度（在图像平面上的大小）可以由像素的大小决定。这样，我们就可以从一个二维图像生成一个粗糙的三维体素网格。

不过问题是，当我们使用三维卷积核去卷积的时候，是牺牲了Z方向的平移不变性的，但是在XY方向上仍然有很好的平移不变性

然而，这个方法有一些限制。首先，它假设图像中的每个像素都对应于一个体素管，这在许多情况下可能并不成立。例如，对于遮挡和反射等现象，这个方法可能无法处理。其次，这个方法生成的体素网格可能比较粗糙，不能精确地表示物体的形状。

为了解决这些问题，我们可以使用深度学习的方法，如卷积神经网络和生成对抗网络（GAN），来改进这个过程。这些方法可以学习从二维图像到三维体素网格的映射关系，生成更精确和细致的体素网格。

### 问题

体素表示的问题就是占用空间很多，比如说我们使用32位浮点数来表示体素，那么一个1020x1024x1024大小的体素，就需要使用4GB的空间

![20](./assets/20-1684385999404-27.jpg)

当然也是有解决方法的

### 多分辨率体素：八叉树

这种体素的想法是一个体素网格可以有多重分辨率，这样可以使用一些低分辨率体素去构建主要结构，然后使用高分辨率的体素去构建细节，高分辨率体素可能是低分辨率体素的稀疏子集

![21](./assets/21-1684386684279-29.jpg)

### 多分辨率体素：嵌套形状层

这种方法的思想是，与其表示为密集的体素网格，不如表示为完整三维形状，或者说将对象形状从内到外的表示出来

我们有一些粗糙的外层，然后有一些负体素，我们可以稀疏的表示

我们可以将一个物体表示为不同的稀疏体素层的总和

![22](./assets/22-1684387545840-31.jpg)

## 隐函数法（Implicit Functions）

这是一种隐式表面的想法：我们想将三维形状表示为一个函数，所以我们需要做的就是学习一些函数，输入一些三维空间坐标，就可以输出任何位置被占据的概率

与体素网格这种方法不同，体素网格这种方法就是在空间中一些有限点集上对这样的函数进行采样，然后将这些样本以某种显式网格表示形式存储到函数中，但是我们在这里使用隐式函数，使用这种数学函数本身来隐式地表示这些三维形状，然后我们可以在3D空间中的任意放置点从这个函数中采样，它应该告诉我们这个位置是在对象内部还是外部

![25](/home/robot/Project/ML_DL_CV_with_pytorch/EECS498/assets/25-1684388117845-33.jpg)

在实践中，通常把对象表示为层次分明的点集，比如说物体表面的占用概率就是0.5，我们可以直观的的显示，如上图所示，蓝色代表非常接近1的值，红色代表非常接近0的值，白色区域是0.5水平聚集的地方，代表三维形状的实际表面，或者可以成为带符合的距离函数

其思想是这种函数给出了三维空间中的点到表面的欧几里得距离，距离的正负决定了点在对象内外

当然这个函数可能会非常的复杂，所以我们会使用神经网络去学习这个函数表示

![28](./assets/28.jpg)

## 点云（Point Cloud）

### 概念

点云，实际上就是将物体表示为三维空间中的点集，或者说使用一组点集以某种方式覆盖我们想表示的三维形状的表面，比如说下图中的飞机

与体素网格相比，点云某种程度上更有适应性，如果我们想使用体素网格表示精细的细节，那么分辨率就会很高，但是在点云中，我们可以使用不同的点云密度来改变细节的精细程度

当然，点云也有缺点，如果你想实际提取一些三维形状来可视化，那么基需要某种后处理，因为在数学上，点云中的点是无限小的一个质点，但是在可视化的时候我们必须让每个点变成一个有限大小的球然后进行渲染，才可以在屏幕上可视化显示

![31](./assets/31-1684391372248-36.jpg)

但是点云在神经网络中是一个非常有用的东西，同时也是非常常见的，比如说在自动驾驶中，激光雷达就是收集了周围环境的点云表示，所以在自动驾驶程序中，点云是经常使用的，我们可以根据这些原始点云输入去做出一些决策

### 处理点云输入：PointNet

我们如何去构建一种可以处理点云的神经网络模型呢？答案就是PointNet，这是一种常用的处理点云的神经网络模型，PointNet的关键创新之处在于它能够直接从原始点云数据中学习特征，而无需转换成其他类型的数据表示

PointNet的主要设计思想是设计一个对点云的置换（permutation）和变换（transformation）具有不变性的网络。这意味着无论点云中的点的顺序如何改变，或者如何旋转、平移、缩放点云，PointNet的输出都应该保持一致。

PointNet的网络结构主要由两部分组成：

1. 输入转换网络：这部分的作用是学习一个对齐网络，用于将输入的点云对齐到一个规范化的坐标系统。它包括几个MLP（多层感知机）层和一个max pooling层，最后输出一个转换矩阵，这个矩阵会被应用到输入的点云上。
2. 特征提取网络：这部分的作用是从对齐后的点云中提取全局特征。它也包括几个MLP层和一个max pooling层。max pooling层在所有的点上取最大值，从而实现了对点的置换不变性。

PointNet可以进行点分类，也可以进行点分割。对于点分类，网络最后输出的是一个全局特征，表示整个点云的分类。对于点分割，网络会为每一个输入点输出一个特征，表示这个点的分类。

PointNet具体是怎么工作的呢？我们想输入一组点云，假设有P个点，每个点都有一个三维空间的XYZ位置信息

![32](./assets/32-1684392149574-38.jpg)

我们想做的第一件事就是对形状进行分类，当然我们不希望点在点云中的顺序很重要，或者说我们不希望点在点云中的存储顺序会影响分类结果，实际上存储结构的确也不会有所影响，就有点类似于之前的Transformer结构

首先我们对每个点独立运行一个MLP，然后输出D维的特征向量，一个有P个点的点云就可以得到PxD维的特征矩阵，然后我们使用最大池化方法去提取特征，得到一个统一的D维特征向量，然后使用全连接层进行分类，这是因为最大池化函数并不关心输入张量上点的表示顺序，所以很适合处理这些点云

PointNet虽然简单，但却非常强大，它已经在很多3D视觉任务上取得了很好的效果，如物体分类、语义分割和部分分割等。但是，PointNet也有一些局限性，比如它不能很好地捕捉局部结构信息和点之间的关系，为此后来的研究者也提出了很多改进版本，如PointNet++等。

### 生成点云输出

我们想做的另一件事就是使用RGB图像去生成三维形状的点云输出，我们同样可以使用神经网络来完成这个操作

![33](./assets/33-1684395204022-40.jpg)

### 损失函数

我们总是需要一种损失函数去训练网络，来比较生预测点云和实际标签之间的区别，或者说去比较二者之间的区别，同时需要这个函数是可微的，便于我们反向传播，这个函数就是钝化距离（Chamfer distance）

![38](./assets/38-1684395752986-42.jpg)

我们想输入两组点，橙色和蓝色，然后钝角距离就会告诉我们，这两组点有何不同，实际上这个函数有两部分

第一部分，是对每个蓝色点，寻找最近的橙色点，然后我们将计算二者之间的欧氏距离，然后对所有的距离求和

第二项是一样的内容，不过是对橙色点而言的计算

两项最近邻匹配距离项的总和就是钝角函数，唯一使得损失函数为0的方法就是两个点云完全重合

它有几个特性使得它非常适合用于点云神经网络。

1. **无序性**：点云数据本身是无序的，点的顺序并不影响点云表示的实际物体形状。Chamfer distance 计算的是每个点到另一个点集中最近点的距离，这个计算过程与点的顺序无关，这是一种最近邻操作，这意味着，无论我们如何改变点云中点的顺序，Chamfer distance 的值都不会变，这与点云数据的无序性相符合。
2. **鲁棒性**：Chamfer distance 是一个双向的度量，它既考虑了第一个点集中每个点到第二个点集中最近点的距离，也考虑了第二个点集中每个点到第一个点集中最近点的距离。这意味着，如果一个点集中有一些点在另一个点集中没有对应的近邻点，Chamfer distance 也不会变得非常大。这使得它对噪声和离群点有很好的鲁棒性。
3. **简单易计算**：虽然 Chamfer distance 的直接计算需要对每一对点都计算距离，但是我们可以利用空间数据结构（如KD-tree）或者近似算法（如最近邻搜索）来加速计算，使得它在实践中可以高效地用于大规模的点云数据。

## 三角网格（Triangle Mesh）

### 概念

这是计算机图形学中非常常用的表示，有点类似于点云，因为它是将三维形状表示为三维空间中的一组顶点，在三角网格表示中，三维物体的表面被划分为许多小的三角形面片，这些三角形面片的集合就构成了三角网格。

![41](./assets/41-1684397791158-44.jpg)

每个三角形面片由三个顶点（vertices）和三条边（edges）组成。顶点存储了空间中的坐标位置信息，可能还会包含其他属性信息，如颜色、法线、纹理坐标等。边连接了两个顶点，描述了三角形的形状。除此之外，三角网格还包括了面（faces），即由三条边围成的区域，它们描述了物体的表面。

三角网格的优点在于它可以准确和高效地表示复杂的三维表面（在计算机图形学中常用）。由于三角形是平面的，任何复杂的三维表面都可以通过足够数量的三角形面片来逼近。此外，三角网格还支持多种几何运算，如交、并、差等。

![44](./assets/44-1684397858637-46.jpg)

### 预测三角网格：Pixel2Mesh

当然这也需要一种新网络去处理这种数据，比如说ECCV2018年的一篇新论文，就是一种使用神经网络处理网格的算法，称为Pixel2Mesh，这种网络，输入一个RGB图像，然后输出一个三角网格，给出一个图像中物体的完整三维形状

![46](./assets/46-1684401205502-48.jpg)

当然其中有几个关键的地方

### 迭代精细化

第一个地方就是，我们想构建一个神经网络来输出三角网格，但是实际上很难从头创建一个网格对象，所以我们需要将一些初始网格模板输入到网络中，然后随着网络的学习，这个网格开始变形，最终变为我们所需的输出，所以我们需要将初始的球状或者椭圆体网格作为模板在初始阶段输入网络

![47](./assets/47-1684411557420-50.jpg)

我们会以某种方式去查看初始网格与图像的匹配度，然后不断的对网格进行更新（或者说迭代精细化），最后就可以输出与输入图像的几何形状非常匹配的三角网格

### 图卷积神经网络

第二个地方就是我们需要一种在网格结构数据上可以操作的神经网络，这是因为网格并不是我们常见的规则化数据，在这里，我们使用的就是一种名为图卷积的运算

我们熟悉二维/三维卷积运算，我们通过卷积运算输出一个新的特征图，特征图中的特征取决于输入数据中某个局部感受野或者特征的局部淋雨，然后我们通过卷积核的不断滑动来计算所有的特征输出

但是，我们现在使用的数据，不是空间网格，而是任意图结构，所以我们这里的操作要进行相应的改变，图卷积层的输入是一个图和一个附加到图的每个顶点的特征向量

我们想计算图卷积层的输出的时候，依然延续感受野的概念，每个顶点的新特征向量和输出特征取决于输入图中特征向量到图卷积层的局部感受野，所以我们需要依赖于一种特色的数学形式来计算，输出特征向量$f^\prime_i$取决于输入特征向量和所有相邻顶点的特征向量

![48](./assets/48.jpg)

然后我们将这个卷积函数应用与图中的每个顶点，这类似于图像卷积，这样就可以应用与任意数量顶点的图

我们认为这又是一种非常好的处理网格结构数据的方法，并提出了某种神经网络结构

![50](./assets/50.jpg)

### 对齐

回顾任务，我们想根据一个RGB图像预测三角网格，所以我们需要一些方法将图像信息混合到图卷积网络中，所以我们有了一种对齐的想法

对于网格中每个顶点，我们想从图像中获得某种特征向量，代表图像在该顶点的空间位置上的视觉外观，我们可以完成的是，使用二维卷积网络处理图像来提取特征

![51](./assets/51-1684413745569-54.jpg)

如果我们了解相机的内在特性，我们可以使用某种投影算子将我们的三维三角网格的顶点投影到图像平面上，对于每个顶点投影位置，我们使用双线性插值来从卷积网络输出的特征中进行采样，这样就可以使得每个顶点的特征向量完全对齐到图像平面中该特征的位置，这与我们之前在Mask R-CNN中的RoI对齐运算符的想法是一致的

![52](./assets/52.jpg)

但是我们仍然想对特征向量和二维图像平面中的任意位置进行采样，而不是像RoI对齐哪有在规则网格中进行采样，我们现在要做的是在图像平面的每一点上对所有投影顶点位置的特征向量进行采样，这样才可以将图像信息混合到我们的图形卷积网络中

### 损失函数

最后一件事就是损失函数，但是，同一个形状有多重不同的表示，比如说下图中的一个正方形，可以使用两个或者四个三角形来表示，我们希望我们的损失函数不受我们用三角形表示形状的特定方式的影响，而是取决于基础形状本身，不是取决于表示方式

我们实际上是有方法的，就是沿着网格内部取样，将其转化为点云（如下图所示），然后使用钝化距离来比较两个点云之间的区别

![55](./assets/55-1684415510204-57.jpg)

当然这里区分一下在线采样和离线采样的区别，在线采样是对预测的网格采样，离线采样是对标签网格采样

![59](./assets/59-1684417932026-59.jpg)

当然，我们需要通过左边的采样操作进行反向传播，这又是一个复杂问题

# 形状比较指标

## 概念

我们也需要一些指标，来对比模型效果，便于查看模型是否训练良好，在二维目标检测中，我们会使用交并比来查看，在三维中，我们一样也可以使用类似的方法

![66](./assets/66-1684418944289-61.jpg)

当然，在三维中，交并比可能并没有那么大的意义，所以我们可以使用钝化距离来比较我们的精度，这是因为钝化距离使用了L2距离，对异常值很敏感，很适合用来完成评价

![69](./assets/69-1684418981318-63.jpg)

## F1分数

F1分数（F1 Score）是一种用于评估分类模型性能的统计指标，尤其在数据不平衡的情况下特别有用。它是精确率（Precision）和召回率（Recall）的调和平均值。

F1分数在点云上进行评估，有点类似于钝化距离，我们有两组点云（下图中橙色蓝色的两组），橙色为预测点云，蓝色为真实点云，如果预测点云在某个阈值半径内，那么就被认为是正确的

我们想象一下，每个预测点云周围扩展出一个球体，如果一些真实点云落在李沐，那么这个预测点云基被认为是真实的，在下图例子里面，四个橙色点云中三个是正确的，所以精度是3/4，三个真实点云只有两个被预测正确，那么召回率就是2/3，进行一个几何平均值处理就是F1分数

![73](./assets/73-1684419495405-65.jpg)

- 精确率：在所有预测为正例的样本中，真正为正例的比例。
- 召回率：在所有真正为正例的样本中，被正确预测为正例的比例。

F1分数的公式为：

F1 = 2 * (精确率 * 召回率) / (精确率 + 召回率)

F1分数的值范围在0到1之间。1表示模型的表现最佳，0表示模型的表现最差。由于F1分数同时考虑了精确率和召回率，因此它能够更全面地评估模型的性能，并且对异常值更稳健，尤其在正负样本不平衡的情况下。

然而，F1分数并非在所有情况下都是最佳的评估指标。如果你更关心精确率或召回率，可能需要直接使用这两个指标，或者使用其他权衡两者的方法，如ROC曲线下的面积（AUC-ROC）。

# 相机系统

我们想输入一张图片，然后输出一个三维形状来表示，那么我们必须使用一个坐标系，或者说我们必须使用一个**标准坐标（canonical coordinate）**，这意味着我们对每个对象类别的对象进行排序的时候，某种程度上就固定了标准方向，比如说下图中的椅子，可能Z轴正方向就是座椅前方

![79](./assets/79-1684420228659-67.jpg)

另一种选择是在**视图坐标系（View Coordinates）**下进行预测，这会使得三维坐标系与输入图像对齐，是表示三维形状坐标系的另一个选择，很多人认为这是一种更容易的坐标系

标准坐标系有一个问题，那就是输出特征和输入特征不再对其，视图坐标系的话，在特征位置上，输出总是与输入相对应的，出于这一点，如果相同的网络，使用不同的坐标，那么标准网络更容易过拟合，视图网络泛化性能更好

![81](./assets/81.jpg)

# 数据集

ImageNet如此成功，以至于人们创建一个数据集就想为其起名字，这也就是ShapeNet这么命名的原因

ShapeNet是一个三维模型数据集，提供了五十个类别的五万个模型

![87](./assets/87-1684421013492-70.jpg)

但是，这个数据集是合成的，只有孤立的物体，没有上下文信息，所以需要其他的数据集，也就是Pix3D

它有家具的三维模型，或者说这是真实世界的图像，有更多的上下文信息，但是太小了

# 三维预测：Mesh R-CNN