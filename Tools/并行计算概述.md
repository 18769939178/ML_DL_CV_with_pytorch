# GPU与并行处理架构概念

# 并行处理基础

## 串行处理与并行处理概念

Sequential processing(串行处理)
- 指令/代码块依次执行
- 前一条指令执行结束以后才能执行下一条语句
- 一般来说，当程序有数据依赖or分支等这些情况下需要串行
- 使用场景：复杂的逻辑计算(比如：操作系统)

串行处理的示意图如下，当有多个核心的时候会导致性能浪费

![AutoDriverHeart_TensorRT_L1_21](/home/pengfei/文档/ML_DL_CV_with_pytorch/Tools/assets/AutoDriverHeart_TensorRT_L1_21.png)

如果知道依赖条件，并且使用并行处理，将不同的语句合理分配到不同的core中运行

![AutoDriverHeart_TensorRT_L1_23](/home/pengfei/文档/ML_DL_CV_with_pytorch/Tools/assets/AutoDriverHeart_TensorRT_L1_23.png)

回顾一下，我们在这里都做了哪些事情

- 把没有数据依赖的代码分配到各个core各自执行 (schedule, 调度)
- 把一个大的loop循环给分割成多个小代码，分配到各个core执行(loop optimization)
- 在一个指令彻底执行完以前，如果已经得到了想要得到的数据，可以提前执行下一个指令(pipeling, 流水线）
- 我们管这一系列的行为，称作parallelization (并行化)。我们得到的可以充分利用多核多线程的程序叫做parallelized program(并行程序）

Parallel processing(并行)
- 指令/代码块同时执行
- 充分利用multi-core(多核)的特性，多个core一起去完成一个或多个任务
- 使用场景：科学计算，图像处理，深度学习等等

- Loop parallelization
  - 大部分消耗时间长的程序中，要不然就是在I/O上的内存读写消耗时间上长，要不然就是在loop上。针对loop的并行优化是很重要的一个优化策略
  - 在图像处理/深度学习中很多地方都是用到了循环
  - 比如说：pre/post process (前处理后处理)
    - resize, crop, blur, bgr2rgb, rgb2gray, dbscan, findCounters
  - 再比如说：DNN中的卷积(convolution layer)以及全连接层(Fully connected
    layer)

当然，在并行处理中，不同编程语言的默认的ordering也是不同的，比如说C/C++/Objective-C/Pascal这些是row major的（数组的每一行是连续存储的），Fortran/OpenGL/MATLAB/R/Julia这些是column major的

同时注意一下并行和并发的区别

![AutoDriverHeart_TensorRT_L1_30](/home/pengfei/文档/ML_DL_CV_with_pytorch/Tools/assets/AutoDriverHeart_TensorRT_L1_30.png)

并发是一个CPU来回切换着执行不同的任务，但是切换的速度是非常快的，导致在人看起来是同时运行不同任务

![AutoDriverHeart_TensorRT_L1_31](/home/pengfei/文档/ML_DL_CV_with_pytorch/Tools/assets/AutoDriverHeart_TensorRT_L1_31.png)

1. **进程**：进程是操作系统进行资源分配和调度的一个独立单位。每个进程都有自己的地址空间、内存、数据栈以及其他用于跟踪执行的辅助数据。进程间的通信（IPC）通常需要操作系统的介入，因为它们运行在隔离的内存空间内。
2. **线程**：线程是进程的一个实体，被系统独立调度和分派的基本单位。一个进程可以包含多个线程，它们共享相同的内存空间和资源，但每个线程有自己的执行序列（或称为“控制流”）。线程间的通信相对简单，因为它们共享同一进程的内存空间。

总结来说，线程是进程的子集，一个进程可以有多个线程，它们可以并行执行以提高效率。

### “多核”与“加速比”的关系

1. **多核处理器**：多核处理器意味着有多个处理核心在单个处理器芯片上。这使得计算机可以同时执行多个操作，从而提高性能。
2. **加速比**：加速比通常用来描述并行计算相比于串行计算在执行时间上的减少比例。理想情况下，n核处理器的性能是单核处理器的n倍。但实际上，这种线性加速很难实现，原因包括：
   - **Amdahl定律**：Amdahl定律指出，程序的加速比受到其串行部分的限制。即使你有无限多的处理器，程序的最大加速比也受限于其必须串行执行的部分。
   - **资源争用和管理开销**：多线程并行运行时，它们可能会竞争共享资源（如内存），这会引起性能瓶颈。此外，线程管理和通信也需要时间，这会降低加速比。
   - **数据依赖**：程序中的某些操作可能需要等待其他操作完成后才能执行，这限制了并行执行的可能性。

因此，双核的加速不一定就是两倍，8核的加速比有时会差于4核。这主要是由于程序的并行性（可以并行执行的程度）、内存带宽、资源争用、线程管理开销等因素影响。简而言之，并行计算的性能增益并非总是线性的，而是取决于多种因素，包括程序的特性和硬件的限制。

## 常见的并行处理

这个有很多种类的并行处理，从底层的编译器优化到应用层的计算图优化等等

![AutoDriverHeart_TensorRT_L1_32](/home/pengfei/文档/ML_DL_CV_with_pytorch/Tools/assets/AutoDriverHeart_TensorRT_L1_32.png)

### SIMD

在SIMD模型中，一个指令流被同时应用到多个数据流上。具体来说，这意味着在一个操作周期内，可以执行单个指令以处理一个数据集合中的多个数据项。这种方式特别适合于需要对大量数据执行重复操作的任务，例如图像和视频处理、科学计算、数字信号处理等。

如下图中，如果我们对每个计算步骤都编写一次重复的指令就会非常麻烦

![AutoDriverHeart_TensorRT_L1_33](/home/pengfei/文档/ML_DL_CV_with_pytorch/Tools/assets/AutoDriverHeart_TensorRT_L1_33.png)

所以在CUDA编程和TensorRT中，以及NVIDIA的tensor core的设计理念中都有SIMD或者更高级的SIMT（Single Instruction, Multiple Thread），SIMT架构允许单个指令流同时控制多个独立的线程执行，这些线程可以独立地处理不同的数据

![AutoDriverHeart_TensorRT_L1_34](/home/pengfei/文档/ML_DL_CV_with_pytorch/Tools/assets/AutoDriverHeart_TensorRT_L1_34.png)

# GPU并行处理

## 关键字

常用关键字

![AutoDriverHeart_TensorRT_L1_39](/home/pengfei/文档/ML_DL_CV_with_pytorch/Tools/assets/AutoDriverHeart_TensorRT_L1_39.png)

## CPU优化方向

CPU主要是处理复杂逻辑运算，所以优化目标与GPU不同，提高线程数和吞吐量带来的效果不大

![AutoDriverHeart_TensorRT_L1_40](/home/pengfei/文档/ML_DL_CV_with_pytorch/Tools/assets/AutoDriverHeart_TensorRT_L1_40.png)

这里我们介绍一下memory latency，CPU绑定的最近的memory就是Cache，我们搜索数据的时候，如果正好在Cache中，就称为cache hit，这是一个很好的情况

但是很多情况下会cache miss，我们就需要去其他地方寻找需要的数据，去更下级的memory寻找数据，比如说去内存中寻找，当memory距离计算资源越近，latency就越短，所以cache miss会很耗时，寻找的时候，CPU core是没有数据的

![AutoDriverHeart_TensorRT_L1_43](/home/pengfei/文档/ML_DL_CV_with_pytorch/Tools/assets/AutoDriverHeart_TensorRT_L1_43.png)

还有一个金字塔图

![AutoDriverHeart_TensorRT_L1_42](/home/pengfei/文档/ML_DL_CV_with_pytorch/Tools/assets/AutoDriverHeart_TensorRT_L1_42.png)

CPU有一些优化方向

![AutoDriverHeart_TensorRT_L1_45](/home/pengfei/文档/ML_DL_CV_with_pytorch/Tools/assets/AutoDriverHeart_TensorRT_L1_45.png)

- **pipeline（流水线执行）**
  - 提高throughput的一种优化，流水线的基本思想是将指令的执行过程划分为几个阶段，每个阶段由不同的硬件组件处理，这样多个指令就可以在不同的阶段同时进行，提高了CPU的效率和执行速度。
  - ![AutoDriverHeart_TensorRT_L1_46](/home/pengfei/文档/ML_DL_CV_with_pytorch/Tools/assets/AutoDriverHeart_TensorRT_L1_46.png)
  - 假设CPU没有流水线，那么就只能串行执行，一个指令的执行需要经过几个步骤（如取指、译码、执行等），在一个步骤完成之前，下一个步骤无法开始
  - 流水线通过将这些步骤分解，允许在一个指令执行的同时，取下一个指令，如下图所示，执行四个指令的效率快了一倍，原来一个指令就四个周期，现在四个指令八个时钟周期
  - ![AutoDriverHeart_TensorRT_L1_48](/home/pengfei/文档/ML_DL_CV_with_pytorch/Tools/assets/AutoDriverHeart_TensorRT_L1_48.png)
- **cache hierarchy（多级缓存）**
  - 可以减少memory latency，起到一个过渡作用
  - ![AutoDriverHeart_TensorRT_L1_49](/home/pengfei/文档/ML_DL_CV_with_pytorch/Tools/assets/AutoDriverHeart_TensorRT_L1_49.png)

- Pre-fetch
  - 这个也是减少memory latency的一种操作，可以根据不同计算的顺序，预先将计算所需的指令和数据读取出来放入cache中
  - ![AutoDriverHeart_TensorRT_L1_50](/home/pengfei/文档/ML_DL_CV_with_pytorch/Tools/assets/AutoDriverHeart_TensorRT_L1_50.png)
  - 当然，碰到分支的话，就容易出问题，所以有Branch-prediction（分支预测）
- **Branch-prediction（分支预测）**
  - 等上一个计算结果出来了，可以预测下一步计算什么，但是会有大量循环周期浪费
  - ![AutoDriverHeart_TensorRT_L1_51](/home/pengfei/文档/ML_DL_CV_with_pytorch/Tools/assets/AutoDriverHeart_TensorRT_L1_51.png)
  - 可以进一步优化，根据以往的Branch走向去预测，失败就rollback，这样就不需要等待太久，最多损失一次rollback周期
  - ![AutoDriverHeart_TensorRT_L1_52](/home/pengfei/文档/ML_DL_CV_with_pytorch/Tools/assets/AutoDriverHeart_TensorRT_L1_52.png)

- **Multi-threading**
  - 让因为数据依赖而cache miss而stall的core做一些其他的事情，可以提高吞吐量
  - ![AutoDriverHeart_TensorRT_L1_53](/home/pengfei/文档/ML_DL_CV_with_pytorch/Tools/assets/AutoDriverHeart_TensorRT_L1_53.png)

由于CPU处理的大多数都是一些复杂逻辑的计算，有大量的分支以及难以预测的分支方向，所以增加core的数量，增加线程数而带来的throughput的收益往往并不是那么高，但是去掉复杂的逻辑计算，去掉分支，把大量的简单运算放在一起的话，是不是就可以最大化的提高throughput呢？
答案是yes，这个就是GPU所做的事情。

## GPU优化方向

GPU的特点就是core非常多，每一个都只能执行简单的计算，目前主要是CUDA core和Tensor core

![AutoDriverHeart_TensorRT_L1_56](/home/pengfei/文档/ML_DL_CV_with_pytorch/Tools/assets/AutoDriverHeart_TensorRT_L1_56.png)

- **设计目的不同**：CUDA核心设计用于处理广泛的通用计算任务，而Tensor核心专门针对深度学习中的特定类型运算进行优化。
- **计算优化**：Tensor核心在处理深度学习特有的大规模矩阵运算时更加高效，而CUDA核心则提供更广泛的适用性。
- **运算类型**：CUDA核心能够执行各种类型的运算，包括图形、物理模拟等，而Tensor核心主要优化了深度学习中的矩阵运算。

GPU的特点
由于throughput非常的高，所以相比与CPU，cache miss所产生的latency对性能的影响比较小GPU主要负责的任务是大规模计算(图像处理、深度学习等等)，所以一旦fetch好了数据以后，就会一直连续处理，并且很少cache miss

## CPU与GPU的分工不同

- CPU
  - 适合复杂逻辑的运算
  - 优化方向在于减少memory latency,相关的技术有，cache hierarchy, pre-fetch, branch-prediction, multi-threading
  - 不同于GPU，CPU硬件上有复杂的分支预测器去实现branch-prediction
  - 由于CPU经常处理复杂的逻辑，过大的增大core的数量并不能很好的提高throughput
- GPU
  - 适合简单单一的大规模运算。比如说科学计算，图像处理，深度学习等等
  - 优化方向在于提高throughput，相关的技术有，multi-threading，warp schedular
  - 不同于CPU，GPU硬件上有复杂的warp schedular去实现多线程的multi-threading
  - 由于GPU经常处理大规模运算，所以在throughput很高的情况下，GPU内部的memory latency上带来的性能损失不是那么明显，然而CPU和GPU间通信时所产生的memory latency需要重视

# 环境配置

不同的操作系统配置软件不一样

![AutoDriverHeart_TensorRT_L1_61](/home/pengfei/文档/ML_DL_CV_with_pytorch/Tools/assets/AutoDriverHeart_TensorRT_L1_61.png)

ubuntu建议使用vscode并且安装两个插件

- Nsight Visual Studio Code Edition
- vscode-cudacpp

在ubuntu下，终端使用指令

```shell
nvidia-smi
```

就会显示显卡和驱动的信息，注意一下显示的CUDA Version表示最大可兼容的CUDA版本，而不是已经安装的CUDA版本

## TensorRT安装

在[NVIDIA官网](https://developer.nvidia.com/tensorrt)下载SDK的压缩包，这是一种比较好的方法，比较推荐Tar格式

建议TensorRT8的版本，而且可以适应11.0-11.7的CUDA版本

## cuDNN安装

建议8.6.0版本的tar格式安装包，然后解压到特定目录