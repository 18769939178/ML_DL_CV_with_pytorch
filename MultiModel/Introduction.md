# 概念

什么是**多模态（MultiModel）**呢？我们首先看一下人类交流的模式，人类之间的交流有很多模式，或者说表达自己的方式，比如说语言、声音、视觉甚至触觉等，包括语言中单词和句子的选择甚至句子背后的意图

![11-777_MMML_Fall22_L1_8](.\assets\11-777_MMML_Fall22_L1_8.png)

所以我们可以这样定义**模态（Modality）**：感知或者表达某些事物的方式；很多研究集中在感知方面，研究如何将模态整合在一起

![11-777_MMML_Fall22_L1_9](.\assets\11-777_MMML_Fall22_L1_9.png)

当然，也会有一些问题，比如说来自于一个传感器的数据，是否有真正意义上的多种模态？或者只有同一种模态的不同表达形式？我们先提出两个概念，**原始模态**和**抽象模态**，前者很接近传感器，比如说麦克风的声音信号、相机的图像数据等，然后就可以对其进行抽象，比如说将其抽象为语言（或者说使用语言描述），甚至加上语义标签（情感标签这种）等

那么什么是多模态呢？字典的定义很简单，就是多种模态，但是科学研究的定义是：对异构和互联数据的研究

![11-777_MMML_Fall22_L1_10](.\assets\11-777_MMML_Fall22_L1_10.png)

其中，异构和互联两个概念是多模态的核心，我们得搞清楚什么的数据的异构和互联

**异构（Heterogeneous）**，表示信息存在不同的表达模式，表示出不同的结构和质量，就类似于在同一个起源线上，同一侧有更多同质模态，比如说两个相机的同一个图像更加同质，两个相机的不同图像有视角差异，此外猫在不同语言中有不同的表示，而视觉和语言就有更大的差异

![11-777_MMML_Fall22_L1_11](.\assets\11-777_MMML_Fall22_L1_11.png)

当然，原始模态在本质上更加异构，抽象模态会更加抽象和简单，也会更同质

那么，我们如何去描述数据的异构呢？这里有一些在第一级看待异构性的方法，也就是考虑模式中的元素

目前，学术界已经接受这个观点：图像几乎就是一个对象列表，这是图像的近似值，可以认为这是图像的一种表示，这种表示由边界框或者感兴趣区域构成；此外我们还有一些离散化的标签表示，也因此在这个原始级别，表示完全不同

如下图所示，图像中有一些物体，图像也有对应的标题，这些模式是不同的，我们需要去描述这些差异

![11-777_MMML_Fall22_L1_13](.\assets\11-777_MMML_Fall22_L1_13.png)

![11-777_MMML_Fall22_L1_14](.\assets\11-777_MMML_Fall22_L1_14.png)

面对这种情况，学术界已经建立了一些技巧来处理这些异构数据，并且将其带入更接近的同质空间，然后进行大量扩散（diffusion），这就是基础的融合方法

此外，这些模态有不同的方式，就会有元素的不同分布，其中就有一些可能非常接近的频率或者说信息编码，并且这些元素会有结构，比如说图像有空间结构，句子有顺序结构，并且语言还有很多隐藏的结构，所以并不是所有的东西都可以看到或者将可观察的结构可视化出来，你必须考虑潜在的结构

![11-777_MMML_Fall22_L1_15](.\assets\11-777_MMML_Fall22_L1_15.png)

此外，信息是抽象的，图像可以在像素级展示，或者对象级展示，元素不止有一种表示方法，所以图像可以由其边缘或者对象表示

![11-777_MMML_Fall22_L1_16](.\assets\11-777_MMML_Fall22_L1_16.png)

同时，噪声也很重要，比如说图像存在遮挡，拼写存在错误，所以必须考虑噪声

![11-777_MMML_Fall22_L1_17](.\assets\11-777_MMML_Fall22_L1_17.png)

最后是任务的相关性，比如说我们有一个特定任务，我们想知道其中的数据到底有多么相关，或者说数据有异质模式的数据，但是如果只是有两种不同的异质模式却没有某种程度的关联，那么就会失去多模态的强大之处，然后没有关联，那么久没办法同步数据和对齐数据，如果数据真的完全独立，任务就会变得很麻烦

![11-777_MMML_Fall22_L1_18](.\assets\11-777_MMML_Fall22_L1_18.png)

所以，数据对齐是多模态中一个重要的组成部分，所以模式之间的联系是什么呢？这是一个重要的概念，在此之前我们先了解模式交互的概念

## 模态交互

我们限定一些，当我们有两种模式的时候，比如说视觉和语言，我们如何研究他们的共性呢？但是如果想研究交互，那么就需要进行某种推理或者识别

![11-777_MMML_Fall22_L1_19](.\assets\11-777_MMML_Fall22_L1_19.png)

交互只有当你对数据做某些事情的时候才会发生，比如说你使用神经网络来运行数据、得到输出，在那一刻你就开始看到数据交互了，因为这时候数据从一种模态转化为另一种模态，这是另一种类型的推理

还有一个问题就是，可以在无监督情况下进行数据交互吗？答案是不同可以，我们可以在无监督条件下研究数据共性，但是想真正研究数据融合如何相互集成的情况，就需要某种任务和推理

首先，一个模态的元素（比如说视觉方面的物体/对象）和另一种模态的元素（比如说语言上的单词），他们彼此对应，彼此等效，就比如说下图中，图片中的茶杯对应标题中的“teacup”，电脑对应“laptop”，这就是模态之间的连接

![11-777_MMML_Fall22_L1_20](.\assets\11-777_MMML_Fall22_L1_20.png)

这里有两种方法来研究，一种是统计方式，这是数据驱动并且自下而上的方式，一种是语义方式，这是自上而下的人类知识的方式

统计方法，主要依赖数据的统计特性，如均值、方差、协方差，这种方式下，如果统计意义上相关，那么就认为彼此相关，不会考虑数据的内在语义

语义方法，主要依赖数据的语义内容，如词义、对象属性，会试图捕获不同模态之间的语义关系，而不仅仅是表面的统计关系，这种方法通常涉及深入理解数据的内容、上下文和结构

![11-777_MMML_Fall22_L1_23](.\assets\11-777_MMML_Fall22_L1_23.png)

当然，还有一种情况就是元素以某种方式相关，而不是完全相同，比如说下图中的情况，杯子是对应“clean”，沙发是对应“room”，这也是一种情况，这种情况下，他们彼此之间不是对应的，但是之间存在关系，并且由于这种关系，他们应该被联系起来，这就是上图中的“Dependency”和“Relationship”

![11-777_MMML_Fall22_L1_22](.\assets\11-777_MMML_Fall22_L1_22.png)

除去模态之间的连接，还有模态之间的交互（Interaction），我们如何对所有类型的多模态交互进行分类呢？一种方法就是忘记交互的细节，我们只需要去看输入和输出即可

![11-777_MMML_Fall22_L1_24](.\assets\11-777_MMML_Fall22_L1_24.png)

首先我们看输出，将其用作一种分类模式之间交互方式的手段，因此，我们可以看上图，电脑和茶杯在视觉和语言上本身并没有交互，然后我们问出一个问题“这是否是室内”，我们可以从视觉和语言上判断这是室内，并且回答“Yes”，这是一种Unimodal方式

单模态冗余（Unimodal Redundancy）是指在多模态数据中，某一模态的数据可能与其他模态的数据存在重叠或冗余的信息。这种冗余可能是因为不同的模态捕捉到了相同的现象或特征。

例如，考虑一个视频分类任务，其中视频包含图像和音频两个模态。如果视频中的人物正在说“你好”，那么图像模态可能会显示人物的嘴巴在移动，而音频模态会捕捉到“你好”的声音。在这种情况下，两个模态都提供了关于人物正在说话的相同信息，这就是单模态冗余。

我们可以根据多模态信息去进行推理，同时根据视觉和语言信息去推断出一个合理的答案，如下图所示，也就是多模态增强（MultiModel Enhancement）的概念，不同模态给予了相同或者相似的答案，如果进行融合，就会相互增强

![11-777_MMML_Fall22_L1_25](.\assets\11-777_MMML_Fall22_L1_25.png)

如果不同模态给予了不同的信息，如下图所示，比如说问是否是客厅，语言上无法得到准确答案，但是视觉上可以，这就是单模态非冗余（Unimodal Non-redundancy），不同模态有不同的信息，但是一种信息是主导的，在这种情况下如何进行整合？

![11-777_MMML_Fall22_L1_26](.\assets\11-777_MMML_Fall22_L1_26.png)

![11-777_MMML_Fall22_L1_27](.\assets\11-777_MMML_Fall22_L1_27.png)

最开始的多模态学习论文，是一篇期刊论文，其中提出了一种十分相关的分类法，这种方法将模态视为信号，如何研究接收器如何进行通信（或者说接受信号的人如何在大脑中进行整合和解释）

![11-777_MMML_Fall22_L1_28](.\assets\11-777_MMML_Fall22_L1_28.png)

这个分类法的第一级是判断信息是冗余的还是非冗余的，假设我们在观察两种信号或者说两种模态信息

如果是冗余的，相当于在分布上没有带来新东西，是等同的，在某种意义上没有增强，如果是非冗余的，会更加有趣一些，两种模式是相对独立的，但是一种模态是主导地位的，会影响另一种模态，比如说增强或者削弱，这种概念称为“Transfer”（迁移），是一种利用在一个任务或模态上学到的知识来帮助另一个任务或模态的学习方法，这种方法也是多模态学习中的核心方法之一

![11-777_MMML_Fall22_L1_29](.\assets\11-777_MMML_Fall22_L1_29.png)

模态交互的基本机制是乘法、加法、非线性、因果逻辑等

![11-777_MMML_Fall22_L1_30](.\assets\11-777_MMML_Fall22_L1_30.png)

# 核心挑战

多模态学习的核心挑战有如下

- 表示学习Representation
- 模态转化Translation
- 对齐Alignment
- 推理Reasoning
- 生成Generation
- Transference

## Representation

第一个就是Representation，也就是表示学习或者说表征学习，这是多模态的一个基本任务，指如何有效地学习和表示来自多个不同模态（如图像、文本、音频等）的数据，以便于后续任务，如分类、检索、生成、对齐等。单模态的表示学习旨在将原始数据转换为更具信息和语义的形式（或者说更高层的表示），捕获数据的内在结构和语义信息，以便计算机能够更好地理解和处理这些数据。而多模态表示学习是指通过利用多模态之间的互补性，剔除模态间的冗余性，从而学习到更好的特征表示。主要包括两大研究方向：**联合表示（Joint Representations）**和**协同表示（Coordinated Representations）**。

1. 联合表示将多个模态的信息一起映射到一个统一的多模态向量空间；

2. 协同表示负责将多模态中的每个模态分别映射到各自的表示空间，但映射后的向量之间满足一定的相关性约束（例如线性相关）。

![11-777_MMML_Fall22_L1_40](.\assets\11-777_MMML_Fall22_L1_40.png)

在这里有local representation的概念，也就是局部表示学习，核心思想是捕捉数据中的局部结构或特征，而不是整体结构。这种表示方法特别适用于图像、文本和声音等数据，因为这些数据通常包含丰富的局部信息。

表示学习方面，有三个子挑战，Fusion/Coordination/Fission，也就是融合/协调和裂变

Fusion就是指输入的模态数量大于输出表示的模态数量，这通常涉及到将不同模态的特征向量结合在一起。

Coordination的目标是学习每个模态的表示，但是不希望模态和表示的数量改变，也就是确保不同模态的数据在时间和空间上的对齐。例如，视频中的音频和图像需要在时间上同步。

Fission是指如何从一个多模态的表示中提取出单一模态的信息。这通常用于那些只需要单一模态信息的任务。

![11-777_MMML_Fall22_L1_41](.\assets\11-777_MMML_Fall22_L1_41.png)

## Alignment

Interconnect是多模态的核心，所以表示学习更多的会涉及跨模态交互，但是要注意，每种模态的数据都有自身的结构，所以如何在不同模态结构之间进行对齐就是一个很大的挑战

![11-777_MMML_Fall22_L1_42](.\assets\11-777_MMML_Fall22_L1_42.png)

在Alignment方面，有三个子挑战

![11-777_MMML_Fall22_L1_43](.\assets\11-777_MMML_Fall22_L1_43.png)

第一个挑战就是找到connection，不同模态之间是如何连接的

第二个挑战就是Aligned Representation，我们不能只做到对齐和寻找connection，也想用其来学校一种新的表示，这种表示可以称为上下文表示（contextualized representation）或者结构化表示（structure representation）

这里就需要提到Transformer，Transformer会进行明确的对齐，目前就是找到那些离散的连接

第三个挑战就是Element，很多模态的元素是不明确的，也就是不ambiguous，

## Reasoning

第三个挑战不仅仅是表示学习，通常通过多个步骤将知识结合起来

![11-777_MMML_Fall22_L1_44](.\assets\11-777_MMML_Fall22_L1_44.png)

我们知道，在神经网络里面，越高的层，学习到的东西越高级，高层学习到的可能是最终目标的一部分，低层学习到的就是边缘和纹理特征这种

在做Reasoning的时候，我们必须考虑整合结构，不止是显式结构，还有潜在的结构，比如说语言有语法，图像可能有某种结构

![11-777_MMML_Fall22_L1_46](.\assets\11-777_MMML_Fall22_L1_46.png)

## Generation

第四个就是生成，不局限于集成，在这里你将学习一个过程来产生原始模态，反映交互结构的交叉模式

![11-777_MMML_Fall22_L1_47](.\assets\11-777_MMML_Fall22_L1_47.png)

## Transference

之前我们讨论过模态之间的交互，一个模态的信息可以影响另一个模态，比如说一个模态有噪声或者信息有限，就需要另一个模态的信息来辅助判断

![11-777_MMML_Fall22_L1_48](.\assets\11-777_MMML_Fall22_L1_48.png)



![11-777_MMML_Fall22_L1_49](.\assets\11-777_MMML_Fall22_L1_49.png)

## Quantification

这里的量化非常有意思

![11-777_MMML_Fall22_L1_50](.\assets\11-777_MMML_Fall22_L1_50.png)

## 核心

多模态的每个问题都有局部表示学习和对齐，同时对齐也带来了结构化的表示，不同模态的信息最终走到一起进行推理

![11-777_MMML_Fall22_L1_51](C:\Users\Michael\Downloads\ML_DL_CV_with_pytorch\MultiModel\assets\11-777_MMML_Fall22_L1_51.png)